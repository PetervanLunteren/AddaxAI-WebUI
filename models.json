{
  "models": {
    "det": [
      {
        "model_id": "MD5A-0-0",
        "friendly_name": "MegaDetector 5A",
        "emoji": "ðŸ…",
        "type": "base",
        "env": "addaxai-base",
        "model_fname": "md_v5a.0.0.pt",
        "description": "MegaDetector is an object detection model that identifies animals, people, and vehicles in camera trap images (which also makes it useful for eliminating blank images). This model is trained on several hundred thousand bounding boxes from a variety of ecosystems. The animals located by this model with be further identified by the species recognition models.",
        "description_short": "Industry standard â€¢ Battle tested",
        "developer": "Dan Morris",
        "info_url": "https://github.com/agentmorris/MegaDetector",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "MD5B-0-0",
        "friendly_name": "MegaDetector 5B",
        "emoji": "ðŸ¦˜",
        "type": "base",
        "env": "addaxai-base",
        "model_fname": "md_v5b.0.0.pt",
        "description": "MegaDetector 5b differs from 5a only in its training data and is also an object detection model that identifies animals, people, and vehicles in camera trap images (which also makes it useful for eliminating blank images). This model is trained on several hundred thousand bounding boxes from a variety of ecosystems. The animals located by this model with be further identified by the species recognition models.",
        "description_short": "Alternative to 5A â€¢ Dataset-dependent performance",
        "developer": "Dan Morris",
        "citation": "https://arxiv.org/abs/1907.06772",
        "info_url": "https://github.com/agentmorris/MegaDetector",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "MD1000-REDWOOD-0-0",
        "friendly_name": "MegaDetector 1000 Redwood",
        "emoji": "ðŸŒ²",
        "type": "base",
        "env": "addaxai-base",
        "model_fname": "md_v1000.0.0-redwood.pt",
        "description": "This model is a new addition to the MegaDetector series, designed to improve accuracy, but still needs to be thoroughly tested by the community. These are object detection models designed to identify animals, people, and vehicles in camera trap images. Animals detected by this model will be passed to species recognition models for further identification. By default, if you're already using MDv5a or MDv5b, keep using it until you have time to do a thorough comparison on your data against MDv1000-redwood. If you're starting from scratch and you want the safe thing that has been studied extensively in the literature, use MDv5a. If you want to join me on the cutting edge and see some bananas-good recall, use MDv1000-redwood. Only use the smaller model MDv1000-spruce if you're sure you need them. Faster sounds better, but in a world where a regular laptop with no GPU can process 50k images per day even through MDv5, and GPUs than can do 1M images per day come standard on home PCs now, the scenario where speed makes a qualitative difference is niche, and if in practice running 2x faster means you need to lower your confidence threshold to preserve recall, you've reduced compute time to increase human time, which is almost never a tradeoff you want to make.",
        "description_short": "Promising new standard â€¢ Being battle tested",
        "developer": "Dan Morris",
        "citation": "https://arxiv.org/abs/1907.06772",
        "info_url": "https://github.com/agentmorris/MegaDetector",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "MD1000-SPRUCE-0-0",
        "friendly_name": "MegaDetector 1000 Spruce",
        "emoji": "ðŸŒ±",
        "type": "base",
        "env": "addaxai-base",
        "model_fname": "md_v1000.0.0-spruce.pt",
        "description": "This model is a new addition to the MegaDetector series, designed to improve processing speed, but still needs to be thoroughly tested by the community. These are object detection models designed to identify animals, people, and vehicles in camera trap images. Animals detected by this model will be passed to species recognition models for further identification. By default, if you're already using MDv5a or MDv5b, keep using it until you have time to do a thorough comparison on your data against MDv1000-redwood. If you're starting from scratch and you want the safe thing that has been studied extensively in the literature, use MDv5a. If you want to join me on the cutting edge and see some bananas-good recall, use MDv1000-redwood. Only use the smaller model MDv1000-spruce if you're sure you need them. Faster sounds better, but in a world where a regular laptop with no GPU can process 50k images per day even through MDv5, and GPUs than can do 1M images per day come standard on home PCs now, the scenario where speed makes a qualitative difference is niche, and if in practice running 2x faster means you need to lower your confidence threshold to preserve recall, you've reduced compute time to increase human time, which is almost never a tradeoff you want to make.",
        "description_short": "Low accuracy (-14%) â€¢ 13x faster than 5A",
        "developer": "Dan Morris",
        "citation": "https://arxiv.org/abs/1907.06772",
        "info_url": "https://github.com/agentmorris/MegaDetector",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "MD1000-CEDAR-0-0",
        "friendly_name": "MegaDetector 1000 Cedar",
        "emoji": "ðŸŒ³",
        "type": "base",
        "env": "addaxai-base",
        "model_fname": "md_v1000.0.0-cedar.pt",
        "description": "This model is a new addition to the MegaDetector series, but still needs to be thoroughly tested by the community. These are object detection models designed to identify animals, people, and vehicles in camera trap images. Animals detected by this model will be passed to species recognition models for further identification. By default, if you're already using MDv5a or MDv5b, keep using it until you have time to do a thorough comparison on your data against MDv1000-redwood. If you're starting from scratch and you want the safe thing that has been studied extensively in the literature, use MDv5a. If you want to join me on the cutting edge and see some bananas-good recall, use MDv1000-redwood. Only use the smaller model MDv1000-spruce if you're sure you need them. Faster sounds better, but in a world where a regular laptop with no GPU can process 50k images per day even through MDv5, and GPUs than can do 1M images per day come standard on home PCs now, the scenario where speed makes a qualitative difference is niche, and if in practice running 2x faster means you need to lower your confidence threshold to preserve recall, you've reduced compute time to increase human time, which is almost never a tradeoff you want to make.",
        "description_short": "High accuracy (-1%) â€¢ 2x faster than 5A",
        "developer": "Dan Morris",
        "citation": "https://arxiv.org/abs/1907.06772",
        "info_url": "https://github.com/agentmorris/MegaDetector",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "MD1000-LARCH-0-0",
        "friendly_name": "MegaDetector 1000 Larch",
        "emoji": "ðŸ‚",
        "type": "base",
        "env": "addaxai-base",
        "model_fname": "md_v1000.0.0-larch.pt",
        "description": "This model is a new addition to the MegaDetector series, designed to improve processing speed, but still needs to be thoroughly tested by the community. These are object detection models designed to identify animals, people, and vehicles in camera trap images. Animals detected by this model will be passed to species recognition models for further identification. By default, if you're already using MDv5a or MDv5b, keep using it until you have time to do a thorough comparison on your data against MDv1000-redwood. If you're starting from scratch and you want the safe thing that has been studied extensively in the literature, use MDv5a. If you want to join me on the cutting edge and see some bananas-good recall, use MDv1000-redwood. Only use the smaller model MDv1000-spruce if you're sure you need them. Faster sounds better, but in a world where a regular laptop with no GPU can process 50k images per day even through MDv5, and GPUs than can do 1M images per day come standard on home PCs now, the scenario where speed makes a qualitative difference is niche, and if in practice running 2x faster means you need to lower your confidence threshold to preserve recall, you've reduced compute time to increase human time, which is almost never a tradeoff you want to make.",
        "description_short": "Good accuracy (-3%) â€¢ 2.4x faster than 5A",
        "developer": "Dan Morris",
        "citation": "https://arxiv.org/abs/1907.06772",
        "info_url": "https://github.com/agentmorris/MegaDetector",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "MD1000-SORREL-0-0",
        "friendly_name": "MegaDetector 1000 Sorrel",
        "emoji": "ðŸŒ¿",
        "type": "base",
        "env": "addaxai-base",
        "model_fname": "md_v1000.0.0-sorrel.pt",
        "description": "This model is a new addition to the MegaDetector series, designed to improve processing speed, but still needs to be thoroughly tested by the community. These are object detection models designed to identify animals, people, and vehicles in camera trap images. Animals detected by this model will be passed to species recognition models for further identification. By default, if you're already using MDv5a or MDv5b, keep using it until you have time to do a thorough comparison on your data against MDv1000-redwood. If you're starting from scratch and you want the safe thing that has been studied extensively in the literature, use MDv5a. If you want to join me on the cutting edge and see some bananas-good recall, use MDv1000-redwood. Only use the smaller model MDv1000-spruce if you're sure you need them. Faster sounds better, but in a world where a regular laptop with no GPU can process 50k images per day even through MDv5, and GPUs than can do 1M images per day come standard on home PCs now, the scenario where speed makes a qualitative difference is niche, and if in practice running 2x faster means you need to lower your confidence threshold to preserve recall, you've reduced compute time to increase human time, which is almost never a tradeoff you want to make.",
        "description_short": "Moderate accuracy (-3%) â€¢ 7x faster than 5A",
        "developer": "Dan Morris",
        "citation": "https://arxiv.org/abs/1907.06772",
        "info_url": "https://github.com/agentmorris/MegaDetector",
        "min_app_version": "0.1.0"
      }
    ],
    "cls": [
      {
        "model_id": "SPECIESNET-v4-0-1-A-v1",
        "friendly_name": "Global (SpeciesNet)",
        "emoji": "ðŸŒ",
        "type": "speciesnet",
        "env": "addaxai-base",
        "model_fname": "always_crop_99710272_22x8_v12_epoch_00148.pt",
        "description": "SpeciesNet is an image classifier designed to accelerate the review of images from motion-triggered wildlife cameras ('camera traps'). This classifier was trained at Google using a large dataset of camera trap images and an EfficientNet V2 M architecture (https://arxiv.org/abs/2104.00574). It is designed to classify images into one of more than 2000 labels, covering diverse animal species, higher-level taxa (like 'mammalia' or 'felidae'), and non-animal classes ('blank', 'vehicle'). The SpeciesNet classifier is typically run in an ensemble with MegaDetector (https://github.com/agentmorris/MegaDetector), an object detector that identifies the locations and types of objects present in camera trap images, categorizing them as animals, humans, or vehicles. The SpeciesNet ensemble combines these two models using a set of heuristics and, optionally, geofencing rules and taxonomic aggregation that take into account the geographical origin of the image to improve the reliability of its species classification. This ensemble is used for image classification in the Wildlife Insights platform (https://www.wildlifeinsights.org/).",
        "description_short": "Global coverage â€¢ 2,000+ classes â€¢ Google Research",
        "developer": "Google",
        "citation": "https://doi.org/10.1049/cvi2.12318",
        "license": "https://github.com/google/cameratrapai/blob/main/LICENSE",
        "info_url": "https://github.com/google/cameratrapai/?tab=readme-ov-file#speciesnet",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "EUR-DF-v1-3",
        "friendly_name": "Europe",
        "emoji": "ðŸ‡ªðŸ‡º",
        "type": "deepfaune-v1.3",
        "env": "pytorch",
        "model_fname": "deepfaune-vit_large_patch14_dinov2.lvd142m.v3.pt",
        "description": "The Deepfaune initiative aims at developing 'artificial intelligence' models to automatically classify species in images and videos collected using camera-traps. The initiative is led by a core academic team from the French 'Centre National de la Recherche Scientifique' (CNRS), in collaboration with more than 50 European partners involved in wildlife research, conservation and management. The Deepfaune models can be run through a custom software freely available on the website, or through other software packages or platforms like EcoAssist. New versions of the model are published regularly, increasing classification accuracy or adding new species to the list of species that can be recognized. More information is available at: https://www.deepfaune.cnrs.fr",
        "description_short": "Deepfaune v1.3 â€¢ 34 classes",
        "developer": "The DeepFaune initiative",
        "citation": "https://doi.org/10.1007/s10344-023-01742-7",
        "license": "https://creativecommons.org/licenses/by-sa/4.0/",
        "info_url": "https://www.deepfaune.cnrs.fr/en/",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "SAH-DRY-ADS-v1",
        "friendly_name": "Sub-Saharan Drylands",
        "emoji": "ðŸŒµ",
        "type": "addax-sdzwa-pt",
        "env": "pytorch",
        "model_fname": "sub_saharan_drylands_v1.pt",
        "description": "The Sub-Saharan Drylands model is a deep learning image classifier trained on > 2.8 million camera trap images from diverse ecosystems across eastern and southern Africa. Covering 328 categories, primarily at the species level, it supports taxonomic fallback, predicting higher-level taxa (e.g., genus or family) when species-level certainty is low. The model is designed for wildlife identification across savannas, dry forests, arid shrublands, and semi-desert habitats. Training data includes images from South Africa, Tanzania, Kenya, Mozambique, Botswana, Namibia, Rwanda, Madagascar, and Uganda. All training images are open-source and available via LILA BC (https://lila.science/).",
        "description_short": "East & Southern Africa savannas â€¢ 328 classes â€¢ Trained on 2.8M images",
        "developer": "Addax Data Science",
        "citation": "https://joss.theoj.org/papers/10.21105/joss.05581",
        "license": "https://creativecommons.org/licenses/by-nc-sa/4.0/",
        "info_url": "https://addaxdatascience.com/",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "TERRAI-NEP-v1",
        "friendly_name": "Terai region Nepal",
        "emoji": "ðŸ‡³ðŸ‡µ",
        "type": "mewc-keras-alex",
        "env": "tensorflow-v2",
        "model_fname": "model.keras",
        "description": "This model aims to identify 10 different species present in the Terai region of Nepal. The model was trained on 2,000 camera trap images per class (mostly from LILA BC, https://lila.science/). The accuracy, precision, recall, and F1 score are all at 90% for the test set (250 images per class), though performance varies somewhat by species. Camera trap images for some species were unavailable so that images of 'substitute' species had to be used, and local images were not available for training as well, i.e. the model's ability to generalize to the target region remains to be tested. The model is based on a pre-trained EfficientNetV2M. It follows MEWC for image preparation (relying on MegaDetector) and model training. The model was designed to support Bengal tiger conservation.",
        "description_short": "10 classes â€¢ Tiger conservation",
        "developer": "Alexander Merdian-Tarko",
        "license": "https://opensource.org/license/mit",
        "info_url": "https://alexvmt.github.io/",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "TAS-BB-v1",
        "friendly_name": "Tasmania",
        "emoji": "ðŸ‡¦ðŸ‡º",
        "type": "mewc-keras",
        "env": "tensorflow-v2",
        "model_fname": "tas_ens_mewc.keras",
        "description": "The MEWC model for Tasmania has been trained on 2.5 million labelled images from 96 classes. It is based on the EfficientNet v2 Small model architecture, initialised with pre-trained ImageNet base weights. The classes include all non-volant terrestrial mammals (native and introduced) that are found in Tasmania, along with over 50 of the most-commonly observed bird species seen on camera traps. Most classes represent species, but there are also some general classes like snake or insect, and an unknown/washed-out special class. Based on held-out test data, the overall classification accuracy and f1 scores are >99%, and for the common species, accuracy typically exceeds 99.5%. The results for the rarest taxa are worse, but still over 90% in almost all cases.",
        "description_short": "96 classes â€¢ Trained on 2.5M images",
        "developer": "Barry Brook",
        "citation": "https://ecoevorxiv.org/repository/view/6405/",
        "license": "https://creativecommons.org/licenses/by/4.0/",
        "info_url": "https://github.com/zaandahl/mewc",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "NAM-ADS-v1",
        "friendly_name": "Namibian Desert",
        "emoji": "ðŸ‡³ðŸ‡¦",
        "type": "addax-yolov8",
        "env": "pytorch",
        "model_fname": "namib_desert_v1.pt",
        "description": "Model to identify 30 species or higher level taxons present in the desert biome of the Skeleton Coast National Park, North Namibia. The model was trained on a set of more than 850,000 images.",
        "description_short": "Namibian Skeleton Coast â€¢ 30 classes â€¢ Trained on 850k images",
        "developer": "Addax Data Science",
        "citation": "https://joss.theoj.org/papers/10.21105/joss.05581",
        "license": "https://creativecommons.org/licenses/by-nc-sa/4.0/",
        "info_url": "https://addaxdatascience.com/projects/2023-01-dlc/",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "NZI-ADS-v1",
        "friendly_name": "New Zealand invasives",
        "emoji": "ðŸ‡³ðŸ‡¿",
        "type": "addax-yolov8",
        "env": "pytorch",
        "model_fname": "new_zealand_v1.pt",
        "description": "Model to identify 17 species or higher-level taxons present in New Zealand. The model was trained on a set of approximately 2 million camera trap images from various projects across the country. These projects were run by multiple organizations and took place in a diverse range of habitats, using a variety of trail camera brands and models. The model has an overall validation accuracy, precision, and recall of 98%. When tested on an out-of-sample test set, the model scored 95%, 96%, and 94%, respectively. The model was designed to expedite the monitoring of New Zealand's invasive species (deer, possum, pig, cat, rodent, and mustelid).",
        "description_short": "Invasive species monitoring â€¢ 17 classes â€¢ Trained on 2M images",
        "developer": "Addax Data Science",
        "owner": "New Zealand Department of Conservation",
        "license": "https://creativecommons.org/licenses/by-nc-sa/4.0/",
        "info_url": "https://www.doc.govt.nz/",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "PAM-SDZWA-v1",
        "friendly_name": "Peruvian Amazon",
        "emoji": "ðŸ‡µðŸ‡ª",
        "type": "sdzwa-peru-amazon",
        "env": "tensorflow-v1",
        "model_fname": "Peru-Amazon_0.86.h5",
        "description": "This model was trained by Mathias Tobler from the San Diego Zoo Wildlife Alliance. Information about the source of the training dataset and model metrics is not available.",
        "description_short": "Peruvian Amazon rainforest â€¢ 53 classes",
        "developer": "San Diego Zoo Wildlife Alliance",
        "license": "https://opensource.org/license/mit",
        "info_url": "https://github.com/conservationtechlab",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "TKM-ADS-v1",
        "friendly_name": "Turkmenistan",
        "emoji": "ðŸ‡¹ðŸ‡²",
        "type": "addax-yolov8",
        "env": "pytorch",
        "model_fname": "tkm_v1.pt",
        "description": "Model to identify 14 species or higher-level taxons present in Southern Turkmenistan. The model was trained on a set of approximately 1 million camera trap images. The model has an overall validation accuracy, precision, and recall of 95%, 93%, and 94%, respectively. The accuracy was not tested on an out-of-sample-dataset since local images were absent.",
        "description_short": "Southern Turkmenistan â€¢ 14 classes â€¢ Trained on 1M images",
        "developer": "Addax Data Science",
        "citation": "https://joss.theoj.org/papers/10.21105/joss.05581",
        "license": "https://creativecommons.org/licenses/by-nc-sa/4.0/",
        "info_url": "https://addaxdatascience.com/",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "KIR-HEX-v1",
        "friendly_name": "Kyrgyzstan",
        "emoji": "ðŸ‡°ðŸ‡¬",
        "type": "hex-data-pt",
        "env": "pytorch",
        "model_fname": "best_model_Fri_Sep__1_18_50_55_2023.pt",
        "description": "This model is dedicated to the classification of the fauna from Kirghizistan. It was developped by Hex Data (https://hex-data.io) on behalf of OSI-Panthera (https://www.osi-panthera.org/). The model was trained on 42k images, with around 4k images per class, provided by the camera traps set up by OSI-Panthera. Class 'vide' (empty, in French), is used to try to set aside the few false negatives returned by MegaDetector. The rest correspond to the scientific name of the family or species detected.",
        "description_short": "Manas v1 â€¢ OSI-Panthera â€¢ Trained on 42k images",
        "developer": "Hex Data",
        "owner": "OSI-Panthera",
        "license": "https://creativecommons.org/licenses/by-nc-sa/4.0/",
        "info_url": "https://www.osi-panthera.org/",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "SWUSA-SDZWA-v3",
        "friendly_name": "Southwest USA",
        "emoji": "ðŸ‡ºðŸ‡¸",
        "type": "sdzwa-pt",
        "env": "pytorch",
        "model_fname": "southwest_v3.pt",
        "description": "This model distinguishes between 27 species native to the Southwest United States. The training data was collected partially by SDZWA and the California Mountain Lion Project, and includes examples from the NACTI, and CCT training datasets. The training data corpus comprises 91662 images. We used a 70/20/10 Train/Val/Test split. The model reached an overall accuracy of 88% on the test set. Created by Kyra Swanson in 2023 (tswanson@sdzwa.org).",
        "description_short": "Version 3 â€¢ 27 species â€¢ Trained on 92k images",
        "developer": "San Diego Zoo Wildlife Alliance",
        "license": "https://opensource.org/license/mit",
        "info_url": "https://github.com/conservationtechlab",
        "min_app_version": "0.1.0"
      },
      {
        "model_id": "GIF-JAP-v0-2",
        "friendly_name": "Gifu region Japan",
        "emoji": "ðŸ‡¯ðŸ‡µ",
        "type": "gifu-wildlife",
        "env": "pytorch",
        "model_fname": "gifu-wildlife_cls_resnet50_v0.2.1.pth",
        "description": "This model classifies 13 species or higher taxonomic groups found in the Kuraiyama Experimental Forest (KEF) of Gifu University. It was trained on approximately 23,000 camera trap images collected through a wildlife monitoring project in KEF. The model was developed to support the efficient monitoring of key wildlife species in central Japan, sika deer, wild boar, Asian black bear, and Japanese serow. The class-wise accuracies for these four representative species are as follows: deer 0.97, boar 0.94, black bear 0.82, and serow 0.88. Please note that the model was trained on data limited to KEF and its surrounding area, and it has not yet undergone a comprehensive accuracy evaluation. The training data is also limited and imbalanced. This model is intended as a prototype for future development of models for classifying wild animals in Japan.The image dataset used to train this model is the same as the one used in Ando et al. (2019). However, data of humans and dogs were not used for training. Ando et al. (2019) Recognition of wildlife using deep learning in images taken by camera traps. Mammalian Science. 59(1):49-60.(In Japanese with English summary)",
        "description_short": "Central Japan â€¢ 13 classes â€¢ Prototype model",
        "developer": "Gifu University",
        "owner": "Masaki Ando",
        "citation": "https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=201902236803626745",
        "license": "https://github.com/gifu-wildlife/MDetToolsForJCameraTraps?tab=MIT-1-ov-file",
        "info_url": "https://github.com/gifu-wildlife/TrainingMdetClassifire",
        "min_app_version": "0.1.0"
      }
    ]
  }
}